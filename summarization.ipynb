{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530f2b64-a6e8-42da-8da6-5ccde1d48bb0",
   "metadata": {},
   "source": [
    "# Summarize Text\n",
    "\n",
    "## Overview\n",
    "\n",
    "LLMs are a great tool for summarizing text content, given their proficiency in understanding and generating text.\n",
    "\n",
    "In the context of [RAG (retrieval-augmented generation)](https://github.com/TCLee/rag-langchain), summarizing text can help distill the information in a large number of retrieved documents to provide context for a LLM.\n",
    "\n",
    "In this notebook, we'll go over how to summarize text content with 3 different strategies. First, we split/chunk a long piece of text into \"sub-documents\". Then, we'll explore the following strategies:\n",
    "- **Stuff**: Simply \"stuff\" all the documents into a single prompt. This is the simplest approach if all the documents can fit inside the model's context window.\n",
    "- **Map-Reduce**: Summarize each document in a `map` step and then `reduce` the summaries into a final summary. The `map` step is typically _parallelized_ over the input documents.\n",
    "- **Iterative Refinement**:\n",
    "    1. Summarize the first document.\n",
    "    2. Refine/update the result based on the next document.\n",
    "    3. Repeat through the sequence of documents until finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170548d-1906-417f-990b-61393fcbf734",
   "metadata": {},
   "source": [
    "## Run models locally\n",
    "\n",
    "Before we begin, please make sure you followed the instructions in the [`README.md`](README.md) to setup [`Ollama`](https://ollama.com/) and [`Llama 3.2`](https://ollama.com/library/llama3.2).\n",
    "\n",
    "Two important benefits of running LLMs locally on your own device are:\n",
    "- **Privacy**: Your data is not sent to a third party, and it is not subject to the terms of service of a commercial service.\n",
    "- **Cost**: There is no inference fee, which is important for token-intensive applications (e.g., summarization, agent simulations)\n",
    "\n",
    "In this notebook, we'll be using `Llama 3.2` with size of `3B` parameters. We'll use `Ollama` to serve the LLM and run inference locally.\n",
    "\n",
    "For more info, see this LangChain guide: [How to run models locally](https://python.langchain.com/docs/how_to/local_llms/).\n",
    "\n",
    "Let's load the model into memory and try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "236597aa-768e-4496-aa51-da027243abaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The first man to walk on the Moon was Neil Armstrong. He stepped out of the lunar module Eagle and onto the Moon\\'s surface on July 20, 1969, during the Apollo 11 mission. Armstrong famously declared, \"That\\'s one small step for man, one giant leap for mankind,\" as he became the first person to set foot on another celestial body.\\n\\nHowever, it\\'s worth noting that while Neil Armstrong was the first person to walk on the Moon, Edwin \"Buzz\" Aldrin also walked on the Moon during the same mission.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2024-10-04T07:20:08.317072Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 18360137125, 'load_duration': 9171645458, 'prompt_eval_count': 34, 'prompt_eval_duration': 581221000, 'eval_count': 112, 'eval_duration': 8579658000}, id='run-50a90817-4af6-4b9e-823a-fea7fa073872-0', usage_metadata={'input_tokens': 34, 'output_tokens': 112, 'total_tokens': 146})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chat_model = ChatOllama(\n",
    "    model=\"llama3.2:3b\", \n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "chat_model.invoke(\"Who was the first man on the moon?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f7d2b-cda6-41b0-b064-f73af46bb1cf",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "\n",
    "Let's load in a sample text file that we will use for summarization.\n",
    "\n",
    "The sample file is a short story by [Edgar Allan Poe](https://en.wikipedia.org/wiki/Edgar_Allan_Poe) titled [\"The Cask of Amontillado\"](https://www.gutenberg.org/ebooks/1063). This and other short stories are freely available on [Project Gutenberg](https://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb38894-329a-434e-b97e-a00c2bb115c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "text_loader = TextLoader(\n",
    "    file_path=\"data/the-cask-of-amontillado.txt\", \n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "docs = text_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a057c35-5dc2-4d7f-a13c-83bf56af23ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cask of Amontillado\n",
      "by Edgar Allan Poe\n",
      "\n",
      "\n",
      "The thousand injuries of Fortunato I had borne as I best could, but\n",
      "when he ventured upon insult, I vowed revenge.  You, who so well know\n",
      "the nature of my soul, will not suppose, however, that I gave utterance\n",
      "to a threat.  _At length_ I would be avenged; this was a point definitely\n",
      "settled--but the very definitiveness with which it was resolved,\n",
      "precluded the idea of risk.  I must not only punish, but punish with\n",
      "impunity.  A wrong is unredressed whe\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    docs[0].page_content[:500]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57525664-866f-4fda-9b57-816f6045eb4f",
   "metadata": {},
   "source": [
    "## Stuff: Summarize text in a single LLM call\n",
    "\n",
    "For models with larger context windows, we can summarize a long document via a single LLM call. Here we use `Llama 3.2` that supports a context length of `128K` tokens.\n",
    "\n",
    "LangChain implements a simple pre-built chain [`create_stuff_documents_chain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html)\n",
    "that \"stuffs\" a prompt with the desired context for summarization (and other purposes). It takes a list of documents, insert them all into a prompt, and pass that prompt to an LLM.\n",
    "\n",
    "> See also: [How to summarize text in a single LLM call](https://python.langchain.com/docs/how_to/summarize_stuff/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33a1e9b1-e09f-4b27-9a83-a1a48313954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents \\\n",
    "    import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \n",
    "         \"Write a concise summary of the following:\\n\\n\"\n",
    "         \"{context}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Instantiate chain\n",
    "chain = create_stuff_documents_chain(\n",
    "    llm=chat_model, \n",
    "    prompt=prompt_template,\n",
    "    document_variable_name='context',\n",
    ")\n",
    "\n",
    "# Invoke chain\n",
    "result = chain.invoke(\n",
    "    input={\"context\": docs}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ae4b2-6e2b-48d8-bcd9-b95ce31d990d",
   "metadata": {},
   "source": [
    "`Llama 3.2` outputs the summary in `Markdown` format. Let's display that in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dbc8dbc-725f-4cc5-92e8-9baee2f3fc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The infamous \"Cask of Amontillado\" by Edgar Allan Poe, a masterclass in atmospheric horror and psychological manipulation.\n",
       "\n",
       "The story revolves around Montresor, who seeks revenge against his acquaintance Fortunato for an unspecified offense. Montresor lures Fortunato into the catacombs beneath his family's castle with promises of rare wine, only to trap him alive and ultimately kill him by walled him up in a tomb.\n",
       "\n",
       "Throughout the narrative, Poe expertly crafts tension and suspense through:\n",
       "\n",
       "1. **Atmosphere**: The damp, musty air of the catacombs creates an eerie setting that immerses the reader.\n",
       "2. **Psychological manipulation**: Montresor's words are laced with sarcasm, irony, and condescension, making Fortunato (and the reader) feel uneasy and complicit in his own demise.\n",
       "3. **Building tension**: Poe skillfully ratchets up the suspense by introducing new sounds, movements, and emotions, keeping the reader on edge.\n",
       "4. **Symbolism**: The Amontillado wine serves as a symbol of Montresor's revenge, while the catacombs represent the darkness within human nature.\n",
       "\n",
       "The story's climax is both shocking and chilling, leaving the reader with a sense of unease and a deeper understanding of the complexities of human psychology.\n",
       "\n",
       "Do you have any specific questions about this tale or would you like to discuss its themes and symbolism?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_markdown\n",
    "\n",
    "display_markdown(result, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76e97e-b5a7-4171-a101-5002d08202ba",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "Note that we can also stream the result token-by-token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ee5442c-f105-4338-9f36-42dbfc81a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| infamous| \"|C|ask| of| Am|ont|ill|ado|\"| by| Edgar| Allan| Poe|,| a| master|class| in| atmospheric| horror| and| psychological| manipulation|.\n",
      "\n",
      "|The| story| revolves| around| Mont|res|or|,| who| seeks| revenge| against| his| acquaintance| Fort|un|ato| for| an| unspecified| offense|.| Mont|res|or| l|ures| Fort|un|ato| into| the| cata|com|bs| beneath| his| family|'s| castle| with| promises| of| rare| wine|,| only| to| trap| him| alive| and| ultimately| kill| him| by| w|alled| him| up| in| a| tomb|.\n",
      "\n",
      "|Throughout| the| narrative|,| Poe| expert|ly| crafts| tension| and| suspense| through|:\n",
      "\n",
      "|1|.| **|At|mos|phere|**:| The| damp|,| must|y| air| of| the| cata|com|bs| creates| an| eerie| setting| that| imm|ers|es| the| reader|.\n",
      "|2|.| **|Psych|ological| manipulation|**:| Mont|res|or|'s| words| are| l|aced| with| sarc|asm|,| irony|,| and| con|desc|ension|,| making| Fort|un|ato| (|and| the| reader|)| feel| uneasy| and| comp|licit| in| his| own| demise|.\n",
      "|3|.| **|Building| tension|**:| Poe| skill|fully| r|atch|ets| up| the| suspense| by| introducing| new| sounds|,| movements|,| and| emotions|,| keeping| the| reader| on| edge|.\n",
      "|4|.| **|Symbol|ism|**:| The| Am|ont|ill|ado| wine| serves| as| a| symbol| of| Mont|res|or|'s| revenge|,| while| the| cata|com|bs| represent| the| darkness| within| human| nature|.\n",
      "\n",
      "|The| story|'s| climax| is| both| shocking| and| chilling|,| leaving| the| reader| with| a| sense| of| une|ase| and| a| deeper| understanding| of| the| complexities| of| human| psychology|.\n",
      "\n",
      "|Do| you| have| any| specific| questions| about| this| tale| or| would| you| like| to| discuss| its| themes| and| symbolism|?||"
     ]
    }
   ],
   "source": [
    "input_dict = {\"context\": docs}\n",
    "\n",
    "for token in chain.stream(input=input_dict):\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060ab29-fd4a-4810-9c25-d1f473868cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
